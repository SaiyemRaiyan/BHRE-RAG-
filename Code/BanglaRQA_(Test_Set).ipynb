{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6jzdzX1MdepX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from datasets import Dataset\n",
        "from rouge_score import rouge_scorer\n",
        "import textstat\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSgVKuZWdg9X"
      },
      "outputs": [],
      "source": [
        "# Set up your API key (replace with your actual API key)\n",
        "GROQ_API_KEY = \"\"\n",
        "\n",
        "HF_TOKEN = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kgLSy1pHd9Xn"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"llama-3.1-8b-instant\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EcKNXimzlGoI",
        "outputId": "0f112c10-6a3b-4f00-c3c2-8e742b8b94cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "base = '/content/drive/MyDrive/BQA'\n",
        "test_file_path = f'{base}/Test.json'\n",
        "validation_file_path = f'{base}/Validation.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hydwYefNlJqf",
        "outputId": "ae6eb396-0124-458a-82f4-b6be9efd2c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test data loaded: 300 samples\n",
            "Validation data loaded: 300 samples\n"
          ]
        }
      ],
      "source": [
        "# Load dataset function\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "# Load datasets\n",
        "test_data = load_dataset(test_file_path)\n",
        "validation_data = load_dataset(validation_file_path)\n",
        "\n",
        "print(f\"Test data loaded: {len(test_data['data'])} samples\")\n",
        "print(f\"Validation data loaded: {len(validation_data['data'])} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MjIeew7RlM7n"
      },
      "outputs": [],
      "source": [
        "# Extract contexts, questions, and answers from the data structure\n",
        "def extract_qa_pairs(data):\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    question_types = []\n",
        "    answer_types = []\n",
        "    is_answerable = []\n",
        "\n",
        "    for article in data['data']:\n",
        "        context = article['context']\n",
        "        for qa in article['qas']:\n",
        "            contexts.append(context)\n",
        "            questions.append(qa['question_text'])\n",
        "            if qa['answers']['answer_text'] and len(qa['answers']['answer_text']) > 0:\n",
        "                answers.append(qa['answers']['answer_text'][0])\n",
        "            else:\n",
        "                answers.append(\"\")\n",
        "            question_types.append(qa.get('question_type', 'factoid'))\n",
        "            answer_types.append(qa['answers'].get('answer_type', 'single span'))\n",
        "            is_answerable.append(len(qa['answers']['answer_text']) > 0 if qa['answers']['answer_text'] else False)\n",
        "\n",
        "    return contexts, questions, answers, question_types, answer_types, is_answerable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QdxgTMCzlQ43",
        "outputId": "54ba8eef-d444-4880-d890-939c76b5bfd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 1493 test questions\n",
            "Extracted 1484 validation questions\n"
          ]
        }
      ],
      "source": [
        "# Extract data\n",
        "test_contexts, test_questions, test_answers, test_question_types, test_answer_types, test_is_answerable = extract_qa_pairs(test_data)\n",
        "val_contexts, val_questions, val_answers, val_question_types, val_answer_types, val_is_answerable = extract_qa_pairs(validation_data)\n",
        "\n",
        "print(f\"Extracted {len(test_questions)} test questions\")\n",
        "print(f\"Extracted {len(val_questions)} validation questions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a5ktFjaXn0IZ",
        "outputId": "bf65dc2b-fcc1-42f7-f8c7-b6f840757e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SAMPLE DATA ===\n",
            "Sample 1:\n",
            "Context: অক্ষয় তৃতীয়া হল চান্দ্র বৈশাখ মাসের শুক্লাতৃতীয়া অর্থাৎ শুক্লপক্ষের তৃতীয়া তিথি। হিন্দু ও জৈন ধর...\n",
            "Question: অক্ষয় তৃতীয়া কী ?\n",
            "Answer: 'চান্দ্র বৈশাখ মাসের শুক্লাতৃতীয়া অর্থাৎ শুক্লপক্ষের তৃতীয়া তিথি'\n",
            "Answer type: ['single span', 'single span']\n",
            "Is answerable: True\n",
            "\n",
            "Sample 2:\n",
            "Context: অক্ষয় তৃতীয়া হল চান্দ্র বৈশাখ মাসের শুক্লাতৃতীয়া অর্থাৎ শুক্লপক্ষের তৃতীয়া তিথি। হিন্দু ও জৈন ধর...\n",
            "Question: এই শুভদিনে কে জন্ম নিয়েছিলেন ?\n",
            "Answer: 'বিষ্ণুর ষষ্ঠ অবতার পরশুরাম'\n",
            "Answer type: ['single span', 'single span']\n",
            "Is answerable: True\n",
            "\n",
            "Sample 3:\n",
            "Context: অক্ষয় তৃতীয়া হল চান্দ্র বৈশাখ মাসের শুক্লাতৃতীয়া অর্থাৎ শুক্লপক্ষের তৃতীয়া তিথি। হিন্দু ও জৈন ধর...\n",
            "Question: এই দিনে মহাভারত রচনা আরম্ভ করেন কারা ?\n",
            "Answer: 'বেদব্যাস ও গণেশ'\n",
            "Answer type: ['single span', 'single span']\n",
            "Is answerable: True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Debug: Show sample data\n",
        "print(\"\\n=== SAMPLE DATA ===\")\n",
        "for i in range(min(3, len(test_questions))):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Context: {test_contexts[i][:100]}...\")\n",
        "    print(f\"Question: {test_questions[i]}\")\n",
        "    print(f\"Answer: '{test_answers[i]}'\")\n",
        "    print(f\"Answer type: {test_answer_types[i]}\")\n",
        "    print(f\"Is answerable: {test_is_answerable[i]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vUZHs7l91D-N"
      },
      "outputs": [],
      "source": [
        "# Token counting function\n",
        "def count_tokens(text):\n",
        "    \"\"\"Count tokens using tiktoken for llama-3\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        return len(encoding.encode(text))\n",
        "    except:\n",
        "        # Fallback: approximate token count\n",
        "        return len(text.split()) * 1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LL1PVIJdlUPY"
      },
      "outputs": [],
      "source": [
        "# Normalization function\n",
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q7W2NPRhlXkQ"
      },
      "outputs": [],
      "source": [
        "# Evaluation metrics\n",
        "def compute_exact_match(prediction, truth):\n",
        "    if not prediction or not truth:\n",
        "        return 0\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    if not prediction or not truth:\n",
        "        return 0\n",
        "\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "\n",
        "    common_tokens = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "    common_tokens = sum(common_tokens.values())\n",
        "\n",
        "    if common_tokens == 0:\n",
        "        return 0\n",
        "\n",
        "    prec = 1.0 * common_tokens / len(pred_tokens)\n",
        "    rec = 1.0 * common_tokens / len(truth_tokens)\n",
        "\n",
        "    return 2 * (prec * rec) / (prec + rec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5Op4G2OLlaH4"
      },
      "outputs": [],
      "source": [
        "def get_updated_f1(prediction, truth):\n",
        "    if not prediction or not truth:\n",
        "        return 0\n",
        "\n",
        "    splitted_prediction = [p.strip() for p in prediction.split(';') if p.strip()]\n",
        "    splitted_truth = [t.strip() for t in truth.split(';') if t.strip()]\n",
        "\n",
        "    if len(splitted_truth) == 0 or len(splitted_prediction) == 0:\n",
        "        return 0\n",
        "\n",
        "    scores = np.zeros([len(splitted_truth), len(splitted_prediction)])\n",
        "    for gold_index, gold_item in enumerate(splitted_truth):\n",
        "        for pred_index, pred_item in enumerate(splitted_prediction):\n",
        "            scores[gold_index, pred_index] = compute_f1(pred_item, gold_item)\n",
        "\n",
        "    max_scores = np.zeros(max(len(splitted_truth), len(splitted_prediction)))\n",
        "    for i in range(min(len(splitted_truth), len(splitted_prediction))):\n",
        "        max_scores[i] = scores[i, i]\n",
        "\n",
        "    return np.mean(max_scores) if len(max_scores) > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BFdA58Glc2I"
      },
      "outputs": [],
      "source": [
        "# Improved API call with rate limiting and retries\n",
        "def call_groq_api(prompt, max_tokens=256, max_retries=5):\n",
        "    \"\"\"Call Groq API with rate limiting and retries\"\"\"\n",
        "    url = \" \"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.1,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"top_p\": 0.9\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
        "\n",
        "            if response.status_code == 429:\n",
        "                # Rate limit exceeded - wait and retry\n",
        "                wait_time = 2 ** attempt  # Exponential backoff\n",
        "                print(f\"Rate limit hit. Waiting {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "            answer = result['choices'][0]['message']['content'].strip()\n",
        "\n",
        "            # Clean up the response\n",
        "            answer = re.sub(r'^Answer:\\s*', '', answer)\n",
        "            answer = re.sub(r'\\\"', '', answer)\n",
        "            answer = answer.strip()\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                return \"\"\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def get_llm_answer(context, question):\n",
        "    \"\"\"Get answer from LLM with optimized prompt\"\"\"\n",
        "    # Optimized prompt for better performance\n",
        "    few_shot_examples = \"\"\"\\\n",
        "প্রসঙ্গ থেকে প্রশ্নের উত্তর দিন। উত্তর শুধুমাত্র প্রসঙ্গ থেকে নিতে হবে।\n",
        "\n",
        "উদাহরণ:\n",
        "প্রসঙ্গ: বাংলাদেশের রাজধানী ঢাকা। ঢাকা একটি বড় শহর।\n",
        "প্রশ্ন: বাংলাদেশের রাজধানী কোথায়?\n",
        "উত্তর: ঢাকা\n",
        "\n",
        "প্রসঙ্গ: সূর্য পূর্বে উঠে পশ্চিমে অস্ত যায়।\n",
        "প্রশ্ন: সূর্য কোথায় অস্ত যায়?\n",
        "উত্তর: পশ্চিমে\n",
        "\n",
        "প্রসঙ্গ: বাঘ জঙ্গলে বাস করে।\n",
        "প্রশ্ন: হাতি কোথায় বাস করে?\n",
        "উত্তর: উত্তর পাওয়া যায়নি\n",
        "\n",
        "এখন উত্তর দিন:\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"{few_shot_examples}\n",
        "প্রসঙ্গ: {context}\n",
        "প্রশ্ন: {question}\n",
        "উত্তর: \"\"\"\n",
        "\n",
        "    # Count tokens and add delay based on token count\n",
        "    token_count = count_tokens(prompt)\n",
        "    if token_count > 1000:\n",
        "        time.sleep(2)  # Longer delay for larger prompts\n",
        "\n",
        "    return call_groq_api(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F1kwn04ilhC4"
      },
      "outputs": [],
      "source": [
        "# Evaluation function with rate limiting\n",
        "def evaluate_model(contexts, questions, answers, answer_types, sample_limit=5):\n",
        "    \"\"\"Evaluate the model with proper rate limiting\"\"\"\n",
        "    results = {\n",
        "        'predictions': [],\n",
        "        'em_scores': [],\n",
        "        'f1_scores': [],\n",
        "        'readability_scores': []\n",
        "    }\n",
        "\n",
        "    # Process each sample with careful rate limiting\n",
        "    for i in tqdm(range(min(sample_limit, len(contexts)))):\n",
        "        context = contexts[i]\n",
        "        question = questions[i]\n",
        "        true_answer = answers[i]\n",
        "\n",
        "        # Get prediction from LLM\n",
        "        prediction = get_llm_answer(context, question)\n",
        "\n",
        "        # Handle unanswerable questions\n",
        "        if \"উত্তর পাওয়া যায়নি\" in prediction or not prediction.strip():\n",
        "            prediction = \"\"\n",
        "\n",
        "        # Calculate metrics\n",
        "        em = compute_exact_match(prediction, true_answer)\n",
        "\n",
        "        if answer_types[i] == 'multiple spans':\n",
        "            f1 = get_updated_f1(prediction, true_answer)\n",
        "        else:\n",
        "            f1 = compute_f1(prediction, true_answer)\n",
        "\n",
        "        # Calculate readability score\n",
        "        readability = textstat.flesch_reading_ease(prediction) if prediction else 0\n",
        "\n",
        "        # Store results\n",
        "        results['predictions'].append(prediction)\n",
        "        results['em_scores'].append(em)\n",
        "        results['f1_scores'].append(f1)\n",
        "        results['readability_scores'].append(readability)\n",
        "\n",
        "        # Add strategic delay to avoid rate limits\n",
        "        if i % 2 == 0:  # Every 2nd request\n",
        "            time.sleep(3)\n",
        "        else:\n",
        "            time.sleep(1)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "x7a9Ca6VlkmY",
        "outputId": "3e6061fe-fac7-459f-f61e-8a43f96fcde1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting evaluation on test set (small subset)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:29<00:00,  5.98s/it]\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation on a smaller subset to avoid rate limits\n",
        "print(\"Starting evaluation on test set (small subset)...\")\n",
        "test_results = evaluate_model(test_contexts, test_questions, test_answers, test_answer_types, sample_limit=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NbPnNGdBpLiB"
      },
      "outputs": [],
      "source": [
        "# Create detailed results dataframe\n",
        "detailed_df = pd.DataFrame({\n",
        "    'Context': test_contexts[:len(test_results['predictions'])],\n",
        "    'Question': test_questions[:len(test_results['predictions'])],\n",
        "    'True_Answer': test_answers[:len(test_results['predictions'])],\n",
        "    'Prediction': test_results['predictions'],\n",
        "    'EM_Score': test_results['em_scores'],\n",
        "    'F1_Score': test_results['f1_scores'],\n",
        "    'Readability': test_results['readability_scores']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jFFAH9rI1702",
        "outputId": "6bb13f55-0c82-4d27-db8c-3966715d2761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detailed results saved to 'llama_banglarqa_test_results.csv'\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "detailed_df.to_csv('llama_banglarqa_test_results.csv', index=False, encoding='utf-8')\n",
        "print(\"\\nDetailed results saved to 'llama_banglarqa_test_results.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance analysis\n",
        "if overall_em > 0 and overall_f1 > 0:\n",
        "    improvement_em = ((overall_em - banglat5_results['EM']) / banglat5_results['EM']) * 100\n",
        "    improvement_f1 = ((overall_f1 - banglat5_results['F1']) / banglat5_results['F1']) * 100\n",
        "\n",
        "    print(f\"\\n=== PERFORMANCE IMPROVEMENT ===\")\n",
        "    print(f\"EM Score Improvement: {improvement_em:.2f}%\")\n",
        "    print(f\"F1 Score Improvement: {improvement_f1:.2f}%\")\n",
        "\n",
        "    if improvement_em > 0 and improvement_f1 > 0:\n",
        "        print(\"SUCCESS: Llama-3.1-8B outperforms BanglaT5!\")\n",
        "    else:\n",
        "        print(\"Llama-3.1-8B needs further optimization\")\n",
        "else:\n",
        "    print(\"\\n No valid scores obtained. Please check API configuration.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
