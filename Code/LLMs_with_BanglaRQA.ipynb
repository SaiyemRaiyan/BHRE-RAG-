{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBtEdG83QG0r"
      },
      "outputs": [],
      "source": [
        "# Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "# login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch accelerate sentencepiece pandas numpy datasets\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ue8EzBHoKZE"
      },
      "source": [
        "# ***Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "0vqIQil7QPkL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import requests\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSBcobAtQVPk",
        "outputId": "3608ca65-5305-419f-db98-7bf3915fd52a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "base = '/content/drive/MyDrive/BQA'\n",
        "test_file_path = f'{base}/Test.json'\n",
        "validation_file_path = f'{base}/Validation.json'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gx6xfFsoQ0b"
      },
      "source": [
        "# ***Load Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga1gjn7mQfqK",
        "outputId": "356a7fa5-69c2-4e03-a86a-131300f01849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test data loaded: 1 samples\n",
            "Validation data loaded: 1 samples\n"
          ]
        }
      ],
      "source": [
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "test_data = load_dataset(test_file_path)\n",
        "validation_data = load_dataset(validation_file_path)\n",
        "\n",
        "print(f\"Test data loaded: {len(test_data)} samples\")\n",
        "print(f\"Validation data loaded: {len(validation_data)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTO_1y4zQ-KL"
      },
      "outputs": [],
      "source": [
        "#API Keys (Groq and Openrouter)\n",
        "\n",
        "API_PROVIDERS = {\n",
        "    \"groq\": {\n",
        "        \"url\": \" \",\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"headers\": {\"Content-Type\": \"application/json\"}\n",
        "    },\n",
        "    \"openrouter\": {\n",
        "        \"url\": \" \",\n",
        "        \"model\": \"meta-llama/llama-3.1-8b-instant\",\n",
        "        \"headers\": {\"Content-Type\": \"application/json\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "API_KEYS = {\n",
        "    \"groq\": \"\",\n",
        "    \"openrouter\": \"\"\n",
        "}\n",
        "current_provider = \"groq\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heVreIiRojFj"
      },
      "source": [
        "# ***Model Responses Functions***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "cVR9WtBwQlJq"
      },
      "outputs": [],
      "source": [
        "def get_llama_response(prompt, max_retries=5):\n",
        "    \"\"\"Get response from Llama 3.1 with multiple fallback providers\"\"\"\n",
        "    global current_provider\n",
        "\n",
        "    providers_tried = set()\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        provider = current_provider\n",
        "        if provider in providers_tried:\n",
        "            # Try next provider\n",
        "            available_providers = [p for p in API_PROVIDERS.keys() if p not in providers_tried]\n",
        "            if available_providers:\n",
        "                provider = available_providers[0]\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        providers_tried.add(provider)\n",
        "\n",
        "        config = API_PROVIDERS[provider]\n",
        "        api_key = API_KEYS[provider]\n",
        "\n",
        "        # Skip if no API key\n",
        "        if api_key == f\"your_{provider}_key_here\" or not api_key:\n",
        "            print(f\"Skipping {provider} - no API key provided\")\n",
        "            continue\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            **config[\"headers\"]\n",
        "        }\n",
        "\n",
        "        if provider == \"openrouter\":\n",
        "            headers[\"HTTP-Referer\"] = \"https://colab.research.google.com\"\n",
        "            headers[\"X-Title\"] = \"BanglaRQA Evaluation\"\n",
        "\n",
        "        payload = {\n",
        "            \"model\": config[\"model\"],\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"\"\"You are a expert Bangla question answering assistant. Answer questions based ONLY on the given context.\n",
        "                    Rules:\n",
        "                    1. Answer in concise Bangla\n",
        "                    2. If the question cannot be answered from context, say \"উত্তর দেওয়া সম্ভব নয়\"\n",
        "                    3. For list questions, provide all items separated by commas\n",
        "                    4. For yes/no questions, answer only \"হ্যাঁ\" or \"না\\\"\"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            \"temperature\": 0.1,\n",
        "            \"max_tokens\": 150,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(config[\"url\"], headers=headers, json=payload, timeout=45)\n",
        "\n",
        "            if response.status_code == 429:\n",
        "                # Rate limited - switch provider and wait\n",
        "                wait_time = min(30, 2 ** attempt)  # Exponential backoff\n",
        "                print(f\"Rate limited on {provider}. Waiting {wait_time}s, then trying next provider...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "\n",
        "            answer = result['choices'][0]['message']['content'].strip()\n",
        "\n",
        "            # If we got a successful response, stick with this provider\n",
        "            current_provider = provider\n",
        "            return answer\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error with {provider}: {e}\")\n",
        "            wait_time = min(60, 5 * (attempt + 1))\n",
        "            time.sleep(wait_time)\n",
        "            continue\n",
        "\n",
        "    print(\"All providers failed. Using fallback empty response.\")\n",
        "    return \"উত্তর দেওয়া সম্ভব নয়\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4KjjXqOoxB0"
      },
      "source": [
        "# ***Generating Answers***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjOqSdX9TySb"
      },
      "outputs": [],
      "source": [
        "def generate_answer(context, question):\n",
        "    \"\"\"Generate answer using Llama 3.1 with optimized prompt\"\"\"\n",
        "    prompt = f\"\"\"প্রসঙ্গ: {context}\n",
        "\n",
        "প্রশ্ন: {question}\n",
        "\n",
        "উপরের প্রসঙ্গ文本টি পড়ুন এবং প্রশ্নের উত্তর দিন। উত্তরটি সংক্ষিপ্ত এবং সঠিক হতে হবে। যদি প্রসঙ্গ উত্তর না থাকে, \"উত্তর দেওয়া সম্ভব নয়\" লিখুন।\"\"\"\n",
        "\n",
        "    try:\n",
        "        answer = get_llama_response(prompt)\n",
        "\n",
        "        # Clean and normalize the answer\n",
        "        answer = re.sub(r'^(উত্তর|Answer|answer|ans|Ans)[:\\s]*', '', answer, flags=re.IGNORECASE)\n",
        "        answer = re.sub(r'[।\\.]\\s*$', '', answer)  # Remove trailing punctuation\n",
        "        answer = answer.strip()\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer: {e}\")\n",
        "        return \"উত্তর দেওয়া সম্ভব নয়\"\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normalize Bangla text\"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[।\\.]\\s*$', '', text)  # Remove trailing punctuation\n",
        "    return text\n",
        "\n",
        "def calculate_em(predicted, ground_truth):\n",
        "    \"\"\"Calculate Exact Match score\"\"\"\n",
        "    predicted = normalize_text(predicted)\n",
        "    ground_truth = normalize_text(ground_truth)\n",
        "\n",
        "    # Handle unanswerable questions\n",
        "    if not ground_truth and (\"সম্ভব নয়\" in predicted or not predicted.strip()):\n",
        "        return 1\n",
        "    if not predicted and not ground_truth:\n",
        "        return 1\n",
        "\n",
        "    return 1 if predicted == ground_truth else 0\n",
        "\n",
        "def calculate_f1(predicted, ground_truth):\n",
        "    \"\"\"Calculate F1 score with better Bangla tokenization\"\"\"\n",
        "    predicted = normalize_text(predicted)\n",
        "    ground_truth = normalize_text(ground_truth)\n",
        "\n",
        "    # Handle unanswerable cases\n",
        "    if not ground_truth and (\"সম্ভব নয়\" in predicted or not predicted.strip()):\n",
        "        return 1.0\n",
        "    if not predicted and not ground_truth:\n",
        "        return 1.0\n",
        "    if not predicted or not ground_truth:\n",
        "        return 0.0\n",
        "\n",
        "    # Better tokenization for Bangla\n",
        "    pred_tokens = re.findall(r'[\\w\\u0980-\\u09FF]+|[^\\w\\s]', predicted)  # Bangla words + punctuation\n",
        "    truth_tokens = re.findall(r'[\\w\\u0980-\\u09FF]+|[^\\w\\s]', ground_truth)\n",
        "\n",
        "    if not pred_tokens or not truth_tokens:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "    precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0\n",
        "    recall = len(common_tokens) / len(truth_tokens) if truth_tokens else 0\n",
        "\n",
        "    # Calculate F1\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * (precision * recall) / (precision + recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjMhriz9o3iL"
      },
      "source": [
        "# **Evaluate Relevant Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "-W7bGKjDURlK"
      },
      "outputs": [],
      "source": [
        "def evaluate_dataset(dataset, sample_size=None):\n",
        "    \"\"\"Evaluate model on the dataset\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Extract the actual data from the dictionary structure\n",
        "    if isinstance(dataset, dict):\n",
        "        # Try common keys that might contain the actual data\n",
        "        data_keys = ['data', 'paragraphs', 'questions', 'items', 'samples']\n",
        "        for key in data_keys:\n",
        "            if key in dataset and isinstance(dataset[key], list):\n",
        "                actual_data = dataset[key]\n",
        "                break\n",
        "        else:\n",
        "            # If no common keys found, look for any list value\n",
        "            for key, value in dataset.items():\n",
        "                if isinstance(value, list):\n",
        "                    actual_data = value\n",
        "                    break\n",
        "            else:\n",
        "                raise ValueError(\"Could not find list data in dataset dictionary\")\n",
        "    else:\n",
        "        actual_data = dataset\n",
        "\n",
        "    # Apply sample size to the actual list data\n",
        "    if sample_size and isinstance(actual_data, list):\n",
        "        actual_data = actual_data[:sample_size]\n",
        "\n",
        "    for i, item in enumerate(tqdm(actual_data, desc=\"Evaluating\")):\n",
        "        # Extract data from the item - handle different possible structures\n",
        "        if isinstance(item, dict):\n",
        "            context = item.get('context', '')\n",
        "            question = item.get('question', '')\n",
        "            ground_truth = item.get('answer', '') if item.get('is_answerable', True) else ''\n",
        "            question_type = item.get('question_type', 'unknown')\n",
        "            answer_type = item.get('answer_type', 'unknown')\n",
        "            is_answerable = item.get('is_answerable', True)\n",
        "        else:\n",
        "            # If it's not a dictionary, use empty values\n",
        "            context = ''\n",
        "            question = ''\n",
        "            ground_truth = ''\n",
        "            question_type = 'unknown'\n",
        "            answer_type = 'unknown'\n",
        "            is_answerable = True\n",
        "\n",
        "        # Generate answer\n",
        "        predicted_answer = generate_answer(context, question)\n",
        "\n",
        "        # Calculate scores\n",
        "        em = calculate_em(predicted_answer, ground_truth)\n",
        "        f1 = calculate_f1(predicted_answer, ground_truth)\n",
        "\n",
        "        results.append({\n",
        "            'context': context[:100] + \"...\" if len(context) > 100 else context,\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'predicted_answer': predicted_answer,\n",
        "            'question_type': question_type,\n",
        "            'answer_type': answer_type,\n",
        "            'is_answerable': is_answerable,\n",
        "            'em': em,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "        # Add delay to avoid rate limiting\n",
        "        time.sleep(2)  # 2 seconds between requests\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQoToRv9o-Jz"
      },
      "source": [
        "# **Results Funtions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "AFXOQI-UYBYd"
      },
      "outputs": [],
      "source": [
        "def analyze_results(results):\n",
        "    \"\"\"Analyze results by question type and answer type\"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Overall scores\n",
        "    overall_em = df['em'].mean() * 100\n",
        "    overall_f1 = df['f1'].mean() * 100\n",
        "\n",
        "    print(f\"Overall EM: {overall_em:.2f}%\")\n",
        "    print(f\"Overall F1: {overall_f1:.2f}%\")\n",
        "    print()\n",
        "\n",
        "\n",
        "        # By question type\n",
        "    print(\"Performance by Question Type:\")\n",
        "    question_types = ['factoid', 'causal', 'confirmation', 'list']\n",
        "    for q_type in question_types:\n",
        "        subset = df[df['question_type'] == q_type]\n",
        "        if len(subset) > 0:\n",
        "            em_score = subset['em'].mean() * 100\n",
        "            f1_score = subset['f1'].mean() * 100\n",
        "            count = len(subset)\n",
        "            print(f\"{q_type} (n={count}): EM={em_score:.2f}%, F1={f1_score:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # By answer type\n",
        "    print(\"Performance by Answer Type:\")\n",
        "    answer_types = ['single span', 'multiple spans', 'yes/no']\n",
        "    for a_type in answer_types:\n",
        "        subset = df[df['answer_type'] == a_type]\n",
        "        if len(subset) > 0:\n",
        "            em_score = subset['em'].mean() * 100\n",
        "            f1_score = subset['f1'].mean() * 100\n",
        "            count = len(subset)\n",
        "            print(f\"{a_type} (n={count}): EM={em_score:.2f}%, F1={f1_score:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Answerable vs Unanswerable\n",
        "    print(\"Performance by Answerability:\")\n",
        "    for answerable in [True, False]:\n",
        "        subset = df[df['is_answerable'] == answerable]\n",
        "        status = \"Answerable\" if answerable else \"Unanswerable\"\n",
        "        em_score = subset['em'].mean() * 100\n",
        "        f1_score = subset['f1'].mean() * 100\n",
        "        count = len(subset)\n",
        "        print(f\"{status} (n={count}): EM={em_score:.2f}%, F1={f1_score:.2f}%\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your API keys before running\n",
        "API_KEYS[\"groq\"] = input(\"Enter your Groq API key: \") or API_KEYS[\"groq\"]\n",
        "API_KEYS[\"openrouter\"] = input(\"Enter your OpenRouter API key (or press enter to skip): \") or API_KEYS[\"openrouter\"]\n",
        "\n",
        "# Test the API connection\n",
        "print(\"Testing API connection...\")\n",
        "test_response = get_llama_response(\"হ্যালো, এটি একটি পরীক্ষা।\")\n",
        "print(f\"API test response: {test_response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Dataset type:\", type(validation_data))\n",
        "if isinstance(validation_data, dict):\n",
        "    print(\"Dictionary keys:\", validation_data.keys())\n",
        "    for key, value in validation_data.items():\n",
        "        if isinstance(value, list):\n",
        "            print(f\"List '{key}' has {len(value)} items\")\n",
        "            if len(value) > 0:\n",
        "                print(\"First item type:\", type(value[0]))\n",
        "                if isinstance(value[0], dict):\n",
        "                    print(\"First item keys:\", value[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DUegG5bpFyb"
      },
      "source": [
        "# **Evaluations for Answerable/Unanswerable Questions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate with careful rate limiting\n",
        "print(\"\\nEvaluating on Validation set...\")\n",
        "validation_results = evaluate_dataset(validation_data, sample_size=10)  # Start small\n",
        "validation_df = analyze_results(validation_results)\n",
        "\n",
        "print(\"\\nEvaluating on Test set...\")\n",
        "test_results = evaluate_dataset(test_data, sample_size=15)\n",
        "test_df = analyze_results(test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLqloyoepSjJ",
        "outputId": "1faea337-12ae-4655-fa53-a7a957db75ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation completed! Results saved to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "def save_results(results, filename):\n",
        "    with open(f'{base}/{filename}', 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "save_results(validation_results, 'llama31_validation_results.json')\n",
        "save_results(test_results, 'llama31_test_results.json')\n",
        "\n",
        "print(\"\\nEvaluation completed! Results saved to Google Drive.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
